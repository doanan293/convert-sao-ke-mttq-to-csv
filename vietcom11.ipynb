{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T03:57:08.580843Z",
     "start_time": "2024-09-18T03:57:08.572088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download PDF file\n",
    "import os\n",
    "from operator import lshift\n",
    "import requests\n",
    "import pandas as pd\n",
    "# Get PDF document\n",
    "# pdf_path = \"./Data/Data_pdf/vietcom11.pdf\"\n",
    "pdf_path = \"./Data/Data_pdf/vietcom11.pdf\"\n",
    "\n",
    "# Download PDF if it doesn't already exist\n",
    "if not os.path.exists(pdf_path):\n",
    "  print(\"File doesn't exist, downloading...\")\n",
    "else:\n",
    "  print(f\"File {pdf_path} exists.\")"
   ],
   "id": "d9de185fa535ae33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ./Data/Data_pdf/vietcom11.pdf exists.\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T03:57:21.154135Z",
     "start_time": "2024-09-18T03:57:08.925348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Requires !pip install PyMuPDF, see: https://github.com/pymupdf/pymupdf\n",
    "import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
    "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm \n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
    "\n",
    "    # Other potential text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "# Open PDF and get lines/pages\n",
    "# Note: this only focuses on text, rather than images/figures etc\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        text = text_formatter(text)\n",
    "        # pages_and_texts.append({\"page_number\": page_number+1,  # adjust page numbers since our PDF starts on page 42\n",
    "        #                         \"page_char_count\": len(text),\n",
    "        #                         \"page_word_count\": len(text.split(\" \")),\n",
    "        #                         \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "        #                         \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "        #                         \"text\": text})\n",
    "        pages_and_texts.append({\"page_number\": page_number+1,  # adjust page numbers since our PDF starts on page 42\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n"
   ],
   "id": "6404c1a05ee98eac",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9194it [00:12, 753.04it/s]\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T03:57:21.176231Z",
     "start_time": "2024-09-18T03:57:21.169744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Strings to remove\n",
    "strings_to_remove = [\"Số tài khoản: 0011001932418 STT NGÀY GIAO  DỊCH SỐ TIỀN GHI  CÓ DIỄN GIẢI \"\n",
    "]\n",
    "\n",
    "# Function to remove specified strings\n",
    "def clean_text(text, strings_to_remove):\n",
    "    for string in strings_to_remove:\n",
    "        text = text.replace(string, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "# Cleaning the texts\n",
    "pages_and_texts = [{'page_number': p['page_number'], 'text': clean_text(p['text'], strings_to_remove)} for p in pages_and_texts]"
   ],
   "id": "9c0ed63c6f9a946d",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T03:57:21.383452Z",
     "start_time": "2024-09-18T03:57:21.348190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re # Extracting text data from the pages\n",
    "text_data = [page['text'] for page in pages_and_texts]\n",
    "\n",
    "# Combining all texts into one string\n",
    "combined_text = ' '.join(text_data)"
   ],
   "id": "9a5a525a4701268d",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T04:00:51.313945Z",
     "start_time": "2024-09-18T04:00:50.261401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a regex pattern to extract the records\n",
    "pattern = r'(\\d{2}/\\d{2}/\\d{4}) +([\\d.]+) (.+?)(?=\\d{2}/\\d{2}/\\d{4}|$)'\n",
    "matches = re.findall(pattern, combined_text)\n",
    "\n",
    "# Convert matches to DataFrame\n",
    "data = []\n",
    "for match in matches:\n",
    "    data.append([match[0], match[1], match[2]])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=['Date', 'Money', 'Description'])\n"
   ],
   "id": "e8c9ad4d0561e4c1",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T04:04:53.636187Z",
     "start_time": "2024-09-18T04:04:53.324902Z"
    }
   },
   "cell_type": "code",
   "source": "df.to_csv(\"./Data/Data_csv/vietcom11.csv\", index=False)",
   "id": "c93583c22d17745e",
   "outputs": [],
   "execution_count": 55
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
